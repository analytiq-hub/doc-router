{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../../..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import analytiq_data as ad\n",
    "import asyncio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ad.common.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the MONGODB_URI environment variable\n",
    "os.environ[\"MONGODB_URI\"] = \"mongodb://localhost:27017\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analytiq_client = ad.common.get_analytiq_client(env=\"test\")\n",
    "\n",
    "llm_key = await ad.llm.get_llm_key(analytiq_client, llm_provider=\"mistral\")\n",
    "print(llm_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "org_id = \"6795345439604beca2b2808d\"\n",
    "prompt_id, version = await ad.common.get_prompt_id(analytiq_client, \"cv\", org_id)\n",
    "prompt_id, version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_revid = await ad.common.get_prompt_revid(analytiq_client, prompt_id, version)\n",
    "prompt_revid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_revid = \"68926f54b175d0e0ff51b11c\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_id = \"687c5bd2eecf1be4639ea6d0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gpt-4o-mini'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await ad.llm.get_llm_model(analytiq_client, prompt_revid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear the AWS credentials in the env to avoid using them\n",
    "os.environ[\"AWS_ACCESS_KEY_ID\"] = \"\"\n",
    "os.environ[\"AWS_SECRET_ACCESS_KEY\"] = \"\"\n",
    "os.environ[\"AWS_REGION_NAME\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-24 15:45:10,647 - analytiq_data.llm.llm - INFO - Running new LLM analysis for doc_id/prompt_revid 687c5bd2eecf1be4639ea6d0/688ae98796b08198647ef46d\n",
      "2025-12-24 15:45:10,649 - analytiq_data.llm.llm - INFO - 687c5bd2eecf1be4639ea6d0/688ae98796b08198647ef46d: LLM model: xai/grok-4-1-fast-reasoning, provider: xai, api_key: xai-gFcAVAGP062t********\n",
      "2025-12-24 15:45:10,658 - analytiq_data.llm.llm - INFO - 687c5bd2eecf1be4639ea6d0/688ae98796b08198647ef46d: Attaching OCR and PDF to prompt using base64 for xai\n",
      "2025-12-24 15:45:10,659 - analytiq_data.llm.llm - INFO - 687c5bd2eecf1be4639ea6d0/688ae98796b08198647ef46d: Response format: {'type': 'json_schema', 'json_schema': {'name': 'document_extraction', 'schema': {'type': 'object', 'properties': {'demographics': {'type': 'object', 'properties': {'first_name': {'type': 'string', 'description': 'Candidate first name'}, 'last_name': {'type': 'string', 'description': 'Candidate last name'}, 'address': {'type': 'string', 'description': 'Candidate address.'}, 'linkedin_url': {'type': 'string', 'description': 'The LinkedIn URL, which could be of the form linkedin.com/<user>'}}, 'additionalProperties': False, 'required': ['first_name', 'last_name', 'address', 'linkedin_url'], 'description': 'Demographics'}, 'programming_languages': {'type': 'array', 'items': {'type': 'string'}, 'description': 'Programming languages the candidate is able to program in'}, 'spoken_languages': {'type': 'array', 'items': {'type': 'string'}, 'description': 'spoken languages'}}, 'required': ['demographics', 'programming_languages', 'spoken_languages'], 'additionalProperties': False}, 'strict': True}}\n",
      "\u001b[92m15:45:10 - LiteLLM:INFO\u001b[0m: utils.py:3476 - \n",
      "LiteLLM completion() model= grok-4-1-fast-reasoning; provider = xai\n",
      "2025-12-24 15:45:10,660 - LiteLLM - INFO - \n",
      "LiteLLM completion() model= grok-4-1-fast-reasoning; provider = xai\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "ename": "BadRequestError",
     "evalue": "litellm.BadRequestError: XaiException - Failed to deserialize the JSON body into the target type: messages[1]: data did not match any variant of untagged enum Content at line 1 column 442856",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mHTTPStatusError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/build/analytiq/doc-router/.venv/lib64/python3.13/site-packages/litellm/llms/custom_httpx/llm_http_handler.py:157\u001b[39m, in \u001b[36mBaseLLMHTTPHandler._make_common_async_call\u001b[39m\u001b[34m(self, async_httpx_client, provider_config, api_base, headers, data, timeout, litellm_params, logging_obj, stream, signed_json_body)\u001b[39m\n\u001b[32m    156\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m157\u001b[39m     response = \u001b[38;5;28;01mawait\u001b[39;00m async_httpx_client.post(\n\u001b[32m    158\u001b[39m         url=api_base,\n\u001b[32m    159\u001b[39m         headers=headers,\n\u001b[32m    160\u001b[39m         data=(\n\u001b[32m    161\u001b[39m             signed_json_body\n\u001b[32m    162\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m signed_json_body \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    163\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m json.dumps(data)\n\u001b[32m    164\u001b[39m         ),\n\u001b[32m    165\u001b[39m         timeout=timeout,\n\u001b[32m    166\u001b[39m         stream=stream,\n\u001b[32m    167\u001b[39m         logging_obj=logging_obj,\n\u001b[32m    168\u001b[39m     )\n\u001b[32m    169\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m httpx.HTTPStatusError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/build/analytiq/doc-router/.venv/lib64/python3.13/site-packages/litellm/litellm_core_utils/logging_utils.py:190\u001b[39m, in \u001b[36mtrack_llm_api_timing.<locals>.decorator.<locals>.async_wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    189\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m190\u001b[39m     result = \u001b[38;5;28;01mawait\u001b[39;00m func(*args, **kwargs)\n\u001b[32m    191\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/build/analytiq/doc-router/.venv/lib64/python3.13/site-packages/litellm/llms/custom_httpx/http_handler.py:464\u001b[39m, in \u001b[36mAsyncHTTPHandler.post\u001b[39m\u001b[34m(self, url, data, json, params, headers, timeout, stream, logging_obj, files, content)\u001b[39m\n\u001b[32m    462\u001b[39m     \u001b[38;5;28msetattr\u001b[39m(e, \u001b[33m\"\u001b[39m\u001b[33mstatus_code\u001b[39m\u001b[33m\"\u001b[39m, e.response.status_code)\n\u001b[32m--> \u001b[39m\u001b[32m464\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m    465\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/build/analytiq/doc-router/.venv/lib64/python3.13/site-packages/litellm/llms/custom_httpx/http_handler.py:420\u001b[39m, in \u001b[36mAsyncHTTPHandler.post\u001b[39m\u001b[34m(self, url, data, json, params, headers, timeout, stream, logging_obj, files, content)\u001b[39m\n\u001b[32m    419\u001b[39m response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.client.send(req, stream=stream)\n\u001b[32m--> \u001b[39m\u001b[32m420\u001b[39m \u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    421\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/build/analytiq/doc-router/.venv/lib64/python3.13/site-packages/httpx/_models.py:829\u001b[39m, in \u001b[36mResponse.raise_for_status\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    828\u001b[39m message = message.format(\u001b[38;5;28mself\u001b[39m, error_type=error_type)\n\u001b[32m--> \u001b[39m\u001b[32m829\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m HTTPStatusError(message, request=request, response=\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[31mHTTPStatusError\u001b[39m: Client error '422 Unprocessable Entity' for url 'https://api.x.ai/v1/chat/completions'\nFor more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/422",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mOpenAIError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/build/analytiq/doc-router/.venv/lib64/python3.13/site-packages/litellm/main.py:609\u001b[39m, in \u001b[36macompletion\u001b[39m\u001b[34m(model, messages, functions, function_call, timeout, temperature, top_p, n, stream, stream_options, stop, max_tokens, max_completion_tokens, modalities, prediction, audio, presence_penalty, frequency_penalty, logit_bias, user, response_format, seed, tools, tool_choice, parallel_tool_calls, logprobs, top_logprobs, deployment_id, reasoning_effort, verbosity, safety_identifier, service_tier, base_url, api_version, api_key, model_list, extra_headers, thinking, web_search_options, shared_session, **kwargs)\u001b[39m\n\u001b[32m    608\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m asyncio.iscoroutine(init_response):\n\u001b[32m--> \u001b[39m\u001b[32m609\u001b[39m     response = \u001b[38;5;28;01mawait\u001b[39;00m init_response\n\u001b[32m    610\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/build/analytiq/doc-router/.venv/lib64/python3.13/site-packages/litellm/llms/custom_httpx/llm_http_handler.py:289\u001b[39m, in \u001b[36mBaseLLMHTTPHandler.async_completion\u001b[39m\u001b[34m(self, custom_llm_provider, provider_config, api_base, headers, data, timeout, model, model_response, logging_obj, messages, optional_params, litellm_params, encoding, api_key, client, json_mode, signed_json_body, shared_session)\u001b[39m\n\u001b[32m    287\u001b[39m     async_httpx_client = client\n\u001b[32m--> \u001b[39m\u001b[32m289\u001b[39m response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_common_async_call(\n\u001b[32m    290\u001b[39m     async_httpx_client=async_httpx_client,\n\u001b[32m    291\u001b[39m     provider_config=provider_config,\n\u001b[32m    292\u001b[39m     api_base=api_base,\n\u001b[32m    293\u001b[39m     headers=headers,\n\u001b[32m    294\u001b[39m     data=data,\n\u001b[32m    295\u001b[39m     timeout=timeout,\n\u001b[32m    296\u001b[39m     litellm_params=litellm_params,\n\u001b[32m    297\u001b[39m     stream=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    298\u001b[39m     logging_obj=logging_obj,\n\u001b[32m    299\u001b[39m     signed_json_body=signed_json_body,\n\u001b[32m    300\u001b[39m )\n\u001b[32m    301\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m provider_config.transform_response(\n\u001b[32m    302\u001b[39m     model=model,\n\u001b[32m    303\u001b[39m     raw_response=response,\n\u001b[32m   (...)\u001b[39m\u001b[32m    312\u001b[39m     json_mode=json_mode,\n\u001b[32m    313\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/build/analytiq/doc-router/.venv/lib64/python3.13/site-packages/litellm/llms/custom_httpx/llm_http_handler.py:182\u001b[39m, in \u001b[36mBaseLLMHTTPHandler._make_common_async_call\u001b[39m\u001b[34m(self, async_httpx_client, provider_config, api_base, headers, data, timeout, litellm_params, logging_obj, stream, signed_json_body)\u001b[39m\n\u001b[32m    181\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m182\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_handle_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m=\u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprovider_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprovider_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    183\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/build/analytiq/doc-router/.venv/lib64/python3.13/site-packages/litellm/llms/custom_httpx/llm_http_handler.py:3617\u001b[39m, in \u001b[36mBaseLLMHTTPHandler._handle_error\u001b[39m\u001b[34m(self, e, provider_config)\u001b[39m\n\u001b[32m   3611\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m BaseLLMException(\n\u001b[32m   3612\u001b[39m         status_code=status_code,\n\u001b[32m   3613\u001b[39m         message=error_text,\n\u001b[32m   3614\u001b[39m         headers=error_headers,\n\u001b[32m   3615\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m3617\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m provider_config.get_error_class(\n\u001b[32m   3618\u001b[39m     error_message=error_text,\n\u001b[32m   3619\u001b[39m     status_code=status_code,\n\u001b[32m   3620\u001b[39m     headers=error_headers,\n\u001b[32m   3621\u001b[39m )\n",
      "\u001b[31mOpenAIError\u001b[39m: Failed to deserialize the JSON body into the target type: messages[1]: data did not match any variant of untagged enum Content at line 1 column 442856",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mBadRequestError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[28]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m llm_result = \u001b[38;5;28;01mawait\u001b[39;00m ad.llm.run_llm(analytiq_client,\n\u001b[32m      2\u001b[39m                                   document_id=document_id,\n\u001b[32m      3\u001b[39m                                   prompt_revid=prompt_revid,\n\u001b[32m      4\u001b[39m                                   \u001b[38;5;66;03m#llm_model=\"bedrock/anthropic.claude-3-7-sonnet-20250219-v1:0\",\u001b[39;00m\n\u001b[32m      5\u001b[39m                                   \u001b[38;5;66;03m#llm_model=\"anthropic.claude-3-5-sonnet-20240620-v1:0\",\u001b[39;00m\n\u001b[32m      6\u001b[39m                                   \u001b[38;5;66;03m#llm_model=\"groq/deepseek-r1-distill-llama-70b\",\u001b[39;00m\n\u001b[32m      7\u001b[39m                                   \u001b[38;5;66;03m#llm_model=\"gemini/gemini-3-pro-preview\",\u001b[39;00m\n\u001b[32m      8\u001b[39m                                   \u001b[38;5;66;03m#llm_model=\"openrouter/openai/gpt-5.2-chat\",\u001b[39;00m\n\u001b[32m      9\u001b[39m                                   \u001b[38;5;66;03m#llm_model=\"gemini/gemini-2.5-pro\",\u001b[39;00m\n\u001b[32m     10\u001b[39m                                   \u001b[38;5;66;03m#llm_model=\"azure_ai/deepseek-v3\",\u001b[39;00m\n\u001b[32m     11\u001b[39m                                   \u001b[38;5;66;03m#llm_model=\"gpt-5.2\",\u001b[39;00m\n\u001b[32m     12\u001b[39m                                   llm_model=\u001b[33m\"\u001b[39m\u001b[33mxai/grok-4-1-fast-reasoning\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     13\u001b[39m                                   \u001b[38;5;66;03m#llm_model=\"gpt-4o-2024-08-06\",\u001b[39;00m\n\u001b[32m     14\u001b[39m                                   \u001b[38;5;66;03m#llm_model=\"claude-3-5-sonnet\",\u001b[39;00m\n\u001b[32m     15\u001b[39m                                   \u001b[38;5;66;03m#llm_model=\"mistral/mistral-large-latest\",\u001b[39;00m\n\u001b[32m     16\u001b[39m                                   \u001b[38;5;66;03m#llm_model=\"mistral/open-mixtral-8x22b\",\u001b[39;00m\n\u001b[32m     17\u001b[39m                                   force=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     18\u001b[39m llm_result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/build/analytiq/doc-router/packages/python/analytiq_data/notebooks/tests/../../../analytiq_data/llm/llm.py:411\u001b[39m, in \u001b[36mrun_llm\u001b[39m\u001b[34m(analytiq_client, document_id, prompt_revid, llm_model, force)\u001b[39m\n\u001b[32m    408\u001b[39m \u001b[38;5;66;03m# 6. Call the LLM with retry mechanism\u001b[39;00m\n\u001b[32m    409\u001b[39m \u001b[38;5;66;03m# Ensure temperature is valid for the chosen model\u001b[39;00m\n\u001b[32m    410\u001b[39m call_temperature = \u001b[32m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_o_series_model(llm_model) \u001b[38;5;28;01melse\u001b[39;00m \u001b[32m0.1\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m411\u001b[39m response = \u001b[38;5;28;01mawait\u001b[39;00m _litellm_acompletion_with_retry(\n\u001b[32m    412\u001b[39m     model=llm_model,\n\u001b[32m    413\u001b[39m     messages=messages,  \u001b[38;5;66;03m# Use the vision-aware messages\u001b[39;00m\n\u001b[32m    414\u001b[39m     api_key=api_key,\n\u001b[32m    415\u001b[39m     temperature=call_temperature,\n\u001b[32m    416\u001b[39m     response_format=response_format,\n\u001b[32m    417\u001b[39m     aws_access_key_id=aws_access_key_id,\n\u001b[32m    418\u001b[39m     aws_secret_access_key=aws_secret_access_key,\n\u001b[32m    419\u001b[39m     aws_region_name=aws_region_name\n\u001b[32m    420\u001b[39m )\n\u001b[32m    422\u001b[39m \u001b[38;5;66;03m# 7. Get actual usage and cost from LLM response\u001b[39;00m\n\u001b[32m    423\u001b[39m prompt_tokens = response.usage.prompt_tokens\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/build/analytiq/doc-router/.venv/lib64/python3.13/site-packages/stamina/_core.py:898\u001b[39m, in \u001b[36mretry.<locals>.retry_decorator.<locals>.async_inner\u001b[39m\u001b[34m(*args, **kw)\u001b[39m\n\u001b[32m    896\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(wrapped)  \u001b[38;5;66;03m# noqa: RET503\u001b[39;00m\n\u001b[32m    897\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34masync_inner\u001b[39m(*args: P.args, **kw: P.kwargs) -> T:  \u001b[38;5;66;03m# type: ignore[return]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m898\u001b[39m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m attempt \u001b[38;5;129;01min\u001b[39;00m retry_ctx.with_name(name, args, kw):\n\u001b[32m    899\u001b[39m         \u001b[38;5;28;01mwith\u001b[39;00m attempt:\n\u001b[32m    900\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m wrapped(*args, **kw)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/build/analytiq/doc-router/.venv/lib64/python3.13/site-packages/stamina/_core.py:599\u001b[39m, in \u001b[36m_RetryContextIterator.__anext__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    597\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__anext__\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> Attempt:\n\u001b[32m    598\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m Attempt(\n\u001b[32m--> \u001b[39m\u001b[32m599\u001b[39m         \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._t_a_retrying.\u001b[34m__anext__\u001b[39m(),\n\u001b[32m    600\u001b[39m         \u001b[38;5;28mself\u001b[39m._backoff_for_attempt_number,\n\u001b[32m    601\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/build/analytiq/doc-router/.venv/lib64/python3.13/site-packages/tenacity/asyncio/__init__.py:166\u001b[39m, in \u001b[36mAsyncRetrying.__anext__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    164\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__anext__\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> AttemptManager:\n\u001b[32m    165\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m166\u001b[39m         do = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.iter(retry_state=\u001b[38;5;28mself\u001b[39m._retry_state)\n\u001b[32m    167\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m do \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    168\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopAsyncIteration\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/build/analytiq/doc-router/.venv/lib64/python3.13/site-packages/tenacity/asyncio/__init__.py:153\u001b[39m, in \u001b[36mAsyncRetrying.iter\u001b[39m\u001b[34m(self, retry_state)\u001b[39m\n\u001b[32m    151\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    152\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m action \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.iter_state.actions:\n\u001b[32m--> \u001b[39m\u001b[32m153\u001b[39m     result = \u001b[38;5;28;01mawait\u001b[39;00m action(retry_state)\n\u001b[32m    154\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/build/analytiq/doc-router/.venv/lib64/python3.13/site-packages/tenacity/_utils.py:99\u001b[39m, in \u001b[36mwrap_to_async_func.<locals>.inner\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     98\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minner\u001b[39m(*args: typing.Any, **kwargs: typing.Any) -> typing.Any:\n\u001b[32m---> \u001b[39m\u001b[32m99\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/build/analytiq/doc-router/.venv/lib64/python3.13/site-packages/tenacity/__init__.py:400\u001b[39m, in \u001b[36mBaseRetrying._post_retry_check_actions.<locals>.<lambda>\u001b[39m\u001b[34m(rs)\u001b[39m\n\u001b[32m    398\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_post_retry_check_actions\u001b[39m(\u001b[38;5;28mself\u001b[39m, retry_state: \u001b[33m\"\u001b[39m\u001b[33mRetryCallState\u001b[39m\u001b[33m\"\u001b[39m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    399\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m.iter_state.is_explicit_retry \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.iter_state.retry_run_result):\n\u001b[32m--> \u001b[39m\u001b[32m400\u001b[39m         \u001b[38;5;28mself\u001b[39m._add_action_func(\u001b[38;5;28;01mlambda\u001b[39;00m rs: \u001b[43mrs\u001b[49m\u001b[43m.\u001b[49m\u001b[43moutcome\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    401\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m    403\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.after \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib64/python3.13/concurrent/futures/_base.py:449\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    447\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[32m    448\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state == FINISHED:\n\u001b[32m--> \u001b[39m\u001b[32m449\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    451\u001b[39m \u001b[38;5;28mself\u001b[39m._condition.wait(timeout)\n\u001b[32m    453\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib64/python3.13/concurrent/futures/_base.py:401\u001b[39m, in \u001b[36mFuture.__get_result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    399\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    400\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m401\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    403\u001b[39m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[32m    404\u001b[39m         \u001b[38;5;28mself\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/build/analytiq/doc-router/.venv/lib64/python3.13/site-packages/stamina/_core.py:900\u001b[39m, in \u001b[36mretry.<locals>.retry_decorator.<locals>.async_inner\u001b[39m\u001b[34m(*args, **kw)\u001b[39m\n\u001b[32m    898\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m attempt \u001b[38;5;129;01min\u001b[39;00m retry_ctx.with_name(name, args, kw):\n\u001b[32m    899\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m attempt:\n\u001b[32m--> \u001b[39m\u001b[32m900\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m wrapped(*args, **kw)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/build/analytiq/doc-router/packages/python/analytiq_data/notebooks/tests/../../../analytiq_data/llm/llm.py:178\u001b[39m, in \u001b[36m_litellm_acompletion_with_retry\u001b[39m\u001b[34m(model, messages, api_key, temperature, response_format, aws_access_key_id, aws_secret_access_key, aws_region_name)\u001b[39m\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_o_series_model(model):\n\u001b[32m    177\u001b[39m     temperature = \u001b[32m1\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m178\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m litellm.acompletion(\n\u001b[32m    179\u001b[39m     model=model,\n\u001b[32m    180\u001b[39m     messages=messages,\n\u001b[32m    181\u001b[39m     api_key=api_key,\n\u001b[32m    182\u001b[39m     temperature=temperature,\n\u001b[32m    183\u001b[39m     response_format=response_format,\n\u001b[32m    184\u001b[39m     aws_access_key_id=aws_access_key_id,\n\u001b[32m    185\u001b[39m     aws_secret_access_key=aws_secret_access_key,\n\u001b[32m    186\u001b[39m     aws_region_name=aws_region_name\n\u001b[32m    187\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/build/analytiq/doc-router/.venv/lib64/python3.13/site-packages/litellm/utils.py:1666\u001b[39m, in \u001b[36mclient.<locals>.wrapper_async\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m   1664\u001b[39m timeout = _get_wrapper_timeout(kwargs=kwargs, exception=e)\n\u001b[32m   1665\u001b[39m \u001b[38;5;28msetattr\u001b[39m(e, \u001b[33m\"\u001b[39m\u001b[33mtimeout\u001b[39m\u001b[33m\"\u001b[39m, timeout)\n\u001b[32m-> \u001b[39m\u001b[32m1666\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/build/analytiq/doc-router/.venv/lib64/python3.13/site-packages/litellm/utils.py:1512\u001b[39m, in \u001b[36mclient.<locals>.wrapper_async\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m   1509\u001b[39m         print_verbose(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mError while checking max token limit: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m   1511\u001b[39m \u001b[38;5;66;03m# MODEL CALL\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1512\u001b[39m result = \u001b[38;5;28;01mawait\u001b[39;00m original_function(*args, **kwargs)\n\u001b[32m   1513\u001b[39m end_time = datetime.datetime.now()\n\u001b[32m   1514\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _is_streaming_request(\n\u001b[32m   1515\u001b[39m     kwargs=kwargs,\n\u001b[32m   1516\u001b[39m     call_type=call_type,\n\u001b[32m   1517\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/build/analytiq/doc-router/.venv/lib64/python3.13/site-packages/litellm/main.py:628\u001b[39m, in \u001b[36macompletion\u001b[39m\u001b[34m(model, messages, functions, function_call, timeout, temperature, top_p, n, stream, stream_options, stop, max_tokens, max_completion_tokens, modalities, prediction, audio, presence_penalty, frequency_penalty, logit_bias, user, response_format, seed, tools, tool_choice, parallel_tool_calls, logprobs, top_logprobs, deployment_id, reasoning_effort, verbosity, safety_identifier, service_tier, base_url, api_version, api_key, model_list, extra_headers, thinking, web_search_options, shared_session, **kwargs)\u001b[39m\n\u001b[32m    626\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    627\u001b[39m     custom_llm_provider = custom_llm_provider \u001b[38;5;129;01mor\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mopenai\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m628\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[43mexception_type\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    629\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    630\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcustom_llm_provider\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcustom_llm_provider\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    631\u001b[39m \u001b[43m        \u001b[49m\u001b[43moriginal_exception\u001b[49m\u001b[43m=\u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    632\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcompletion_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcompletion_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    633\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextra_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    634\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/build/analytiq/doc-router/.venv/lib64/python3.13/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py:2340\u001b[39m, in \u001b[36mexception_type\u001b[39m\u001b[34m(model, original_exception, custom_llm_provider, completion_kwargs, extra_kwargs)\u001b[39m\n\u001b[32m   2338\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m exception_mapping_worked:\n\u001b[32m   2339\u001b[39m     \u001b[38;5;28msetattr\u001b[39m(e, \u001b[33m\"\u001b[39m\u001b[33mlitellm_response_headers\u001b[39m\u001b[33m\"\u001b[39m, litellm_response_headers)\n\u001b[32m-> \u001b[39m\u001b[32m2340\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m   2341\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2342\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m error_type \u001b[38;5;129;01min\u001b[39;00m litellm.LITELLM_EXCEPTION_TYPES:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/build/analytiq/doc-router/.venv/lib64/python3.13/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py:520\u001b[39m, in \u001b[36mexception_type\u001b[39m\u001b[34m(model, original_exception, custom_llm_provider, completion_kwargs, extra_kwargs)\u001b[39m\n\u001b[32m    518\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m original_exception.status_code == \u001b[32m422\u001b[39m:\n\u001b[32m    519\u001b[39m     exception_mapping_worked = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m520\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m BadRequestError(\n\u001b[32m    521\u001b[39m         message=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexception_provider\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m - \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmessage\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    522\u001b[39m         model=model,\n\u001b[32m    523\u001b[39m         llm_provider=custom_llm_provider,\n\u001b[32m    524\u001b[39m         response=\u001b[38;5;28mgetattr\u001b[39m(original_exception, \u001b[33m\"\u001b[39m\u001b[33mresponse\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[32m    525\u001b[39m         litellm_debug_info=extra_information,\n\u001b[32m    526\u001b[39m         body=\u001b[38;5;28mgetattr\u001b[39m(original_exception, \u001b[33m\"\u001b[39m\u001b[33mbody\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[32m    527\u001b[39m     )\n\u001b[32m    528\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m original_exception.status_code == \u001b[32m429\u001b[39m:\n\u001b[32m    529\u001b[39m     exception_mapping_worked = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[31mBadRequestError\u001b[39m: litellm.BadRequestError: XaiException - Failed to deserialize the JSON body into the target type: messages[1]: data did not match any variant of untagged enum Content at line 1 column 442856"
     ]
    }
   ],
   "source": [
    "llm_result = await ad.llm.run_llm(analytiq_client,\n",
    "                                  document_id=document_id,\n",
    "                                  prompt_revid=prompt_revid,\n",
    "                                  #llm_model=\"bedrock/anthropic.claude-3-7-sonnet-20250219-v1:0\",\n",
    "                                  #llm_model=\"anthropic.claude-3-5-sonnet-20240620-v1:0\",\n",
    "                                  #llm_model=\"groq/deepseek-r1-distill-llama-70b\",\n",
    "                                  #llm_model=\"gemini/gemini-3-pro-preview\",\n",
    "                                  #llm_model=\"openrouter/openai/gpt-5.2-chat\",\n",
    "                                  #llm_model=\"gemini/gemini-2.5-pro\",\n",
    "                                  #llm_model=\"azure_ai/deepseek-v3\",\n",
    "                                  #llm_model=\"gpt-5.2\",\n",
    "                                  llm_model=\"xai/grok-4-1-fast-reasoning\",\n",
    "                                  #llm_model=\"gpt-4o-2024-08-06\",\n",
    "                                  #llm_model=\"claude-3-5-sonnet\",\n",
    "                                  #llm_model=\"mistral/mistral-large-latest\",\n",
    "                                  #llm_model=\"mistral/open-mixtral-8x22b\",\n",
    "                                  force=True)\n",
    "llm_result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_result_id = await ad.llm.save_llm_result(analytiq_client,\n",
    "                                             document_id=document_id,\n",
    "                                             prompt_revid=prompt_revid,\n",
    "                                             llm_result=llm_result)\n",
    "llm_result_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_result = await ad.llm.get_llm_result(analytiq_client,\n",
    "                                         document_id=document_id,\n",
    "                                         prompt_revid=prompt_revi)\n",
    "llm_result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await ad.llm.delete_llm_result(analytiq_client,\n",
    "                                document_id=document_id,\n",
    "                                prompt_revid=prompt_revid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await ad.llm.run_llm_for_prompt_revids(analytiq_client,\n",
    "                                        document_id=document_id,\n",
    "                                        prompt_revids=[prompt_revid],\n",
    "                                        model=\"groq/deepseek-r1-distill-llama-70b\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from litellm.utils import supports_response_schema\n",
    "\n",
    "supports_response_schema(model=\"groq/deepseek-r1-distill-llama-70b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await ad.llm.list_llm_providers(analytiq_client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ad.llm.is_chat_model(\"mistral/mistral-large-latest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ad.llm.get_supported_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ad.llm.get_available_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in ad.llm.get_supported_models():\n",
    "    if not ad.llm.is_supported_model(model):\n",
    "        raise Exception(f\"Model {model} is not supported\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "litellm.models_by_provider.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await ad.llm.providers.setup_llm_providers(analytiq_client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from litellm import completion\n",
    "\n",
    "aws_config = await ad.aws.get_aws_config(analytiq_client)\n",
    "aws_access_key_id = aws_config[\"aws_access_key_id\"]\n",
    "aws_secret_access_key = aws_config[\"aws_secret_access_key\"]\n",
    "aws_region_name = \"us-east-1\"\n",
    "\n",
    "response = completion(\n",
    "  #provider=\"bedrock\",\n",
    "  #model=\"bedrock/anthropic.claude-sonnet-4-20250514-v1:0\",\n",
    "  #model=\"bedrock/us.anthropic.claude-sonnet-4-20250514-v1:0\",\n",
    "  #model=\"bedrock/arn:aws:bedrock:us-east-1:890742589311:inference-profile/claude-sonnet-4-profile\",\n",
    "  #model=\"bedrock/anthropic.claude-3-7-sonnet-20250219-v1:0\",\n",
    "  #model=\"anthropic.claude-3-5-sonnet-20240620-v1:0\",\n",
    "  #model=\"gemini/gemini-2.0-flash\",\n",
    "  #model=\"gemini/gemini-2.5-flash-preview-05-20\",\n",
    "  model=\"gemini/gemini-2.5-pro-preview-06-05\",\n",
    "  messages=[{ \"content\": \"Hello, how are you?\",\"role\": \"user\"}],\n",
    "  #response_format = {\"type\": \"json_object\"},\n",
    "  aws_access_key_id=aws_access_key_id,\n",
    "  aws_secret_access_key=aws_secret_access_key,\n",
    "  aws_region_name=aws_region_name,\n",
    ")\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import litellm\n",
    "from litellm import CustomLLM, completion, get_llm_provider\n",
    "\n",
    "\n",
    "class MyCustomLLM(CustomLLM):\n",
    "    def completion(self, *args, **kwargs) -> litellm.ModelResponse:\n",
    "        return litellm.completion(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=[{\"role\": \"user\", \"content\": \"Hello world\"}],\n",
    "            mock_response=\"Hi!\",\n",
    "        )  # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_custom_llm = MyCustomLLM()\n",
    "\n",
    "litellm.custom_provider_map = [ # ðŸ‘ˆ KEY STEP - REGISTER HANDLER\n",
    "        {\"provider\": \"my-custom-llm\", \"custom_handler\": my_custom_llm}\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = completion(\n",
    "        model=\"my-custom-llm/my-fake-model\",\n",
    "        messages=[{\"role\": \"user\", \"content\": \"Hello world!\"}],\n",
    "    )\n",
    "\n",
    "assert resp.choices[0].message.content == \"Hi!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
