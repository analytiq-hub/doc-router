{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.append(\"../../..\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import analytiq_data as ad\n",
        "import asyncio\n",
        "import litellm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set the MONGODB_URI environment variable\n",
        "os.environ[\"MONGODB_URI\"] = \"mongodb://localhost:27017\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "litellm.model_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "litellm.model_cost[\"j2-light\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "litellm.get_model_info(model=\"groq/deepseek-r1-distill-llama-70b\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "litellm.models_by_provider"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from litellm import get_supported_openai_params\n",
        "\n",
        "response = get_supported_openai_params(model=\"bedrock/anthropic.claude-3\", custom_llm_provider=\"bedrock\")\n",
        "\n",
        "print(response) # [\"max_tokens\", \"tools\", \"tool_choice\", \"stream\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_embedding_models(provider=\"openai\"):\n",
        "    \"\"\"\n",
        "    Get all embedding models for a given provider.\n",
        "    \n",
        "    Args:\n",
        "        provider: The litellm provider name (e.g., \"openai\", \"cohere\", \"azure\")\n",
        "    \n",
        "    Returns:\n",
        "        List of dictionaries, each containing:\n",
        "        - name: Model name\n",
        "        - dimensions: Embedding vector dimensions (output_vector_size)\n",
        "        - output_cost_per_token: Cost per token for output (typically 0 for embeddings)\n",
        "        - output_cost_per_token_batches: Cost per token for batched output (if available)\n",
        "    \"\"\"\n",
        "    models = litellm.models_by_provider.get(provider, [])\n",
        "    embedding_models = []\n",
        "    \n",
        "    for model in models:\n",
        "        try:\n",
        "            model_info = litellm.get_model_info(model)\n",
        "            # Check if this is an embedding model\n",
        "            if model_info.get('mode') == 'embedding':\n",
        "                embedding_models.append({\n",
        "                    'name': model,\n",
        "                    'dimensions': model_info.get('output_vector_size'),\n",
        "                    'output_cost_per_token': model_info.get('output_cost_per_token'),\n",
        "                    'output_cost_per_token_batches': model_info.get('output_cost_per_token_batches')\n",
        "                })\n",
        "        except Exception as e:\n",
        "            # Skip models that can't be queried\n",
        "            pass\n",
        "    \n",
        "    return embedding_models\n",
        "\n",
        "# Example usage\n",
        "embedding_models = get_embedding_models(\"openai\")\n",
        "print(\"OpenAI Embedding Models:\")\n",
        "for model in embedding_models:\n",
        "    print(f\"\\n  Model: {model['name']}\")\n",
        "    print(f\"    Dimensions: {model['dimensions']}\")\n",
        "    print(f\"    Output cost per token: {model['output_cost_per_token']}\")\n",
        "    print(f\"    Output cost per token (batches): {model['output_cost_per_token_batches']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get all OpenAI models\n",
        "openai_models = litellm.models_by_provider.get(\"openai\", [])\n",
        "\n",
        "# Check each model's info to see if it supports embeddings\n",
        "embedding_models = []\n",
        "for model in openai_models:\n",
        "    try:\n",
        "        model_info = litellm.get_model_info(model)\n",
        "        # Check if model supports embeddings (you may need to test or check litellm source)\n",
        "        # For OpenAI, embedding models typically have \"text-embedding\" in the name\n",
        "        if \"embedding\" in model.lower() or model_info.get(\"supports_embeddings\", False):\n",
        "            embedding_models.append(model)\n",
        "    except Exception as e:\n",
        "        # Skip models that can't be queried\n",
        "        pass\n",
        "\n",
        "print(\"OpenAI Embedding Models:\")\n",
        "for model in embedding_models:\n",
        "    print(f\"  - {model}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get the model card for text-embedding-3-small\n",
        "model_card = litellm.get_model_info(\"text-embedding-3-small\")\n",
        "model_card\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def list_embedding_models(provider=\"openai\"):\n",
        "    \"\"\"List all embedding models for a given provider\"\"\"\n",
        "    models = litellm.models_by_provider.get(provider, [])\n",
        "    embedding_models = []\n",
        "    \n",
        "    for model in models:\n",
        "        try:\n",
        "            model_info = litellm.get_model_info(model)\n",
        "            if model_info.get('mode') == 'embedding':\n",
        "                embedding_models.append({\n",
        "                    'model': model,\n",
        "                    'dimensions': model_info.get('output_vector_size'),\n",
        "                    'cost_per_token': model_info.get('input_cost_per_token'),\n",
        "                    'max_tokens': model_info.get('max_input_tokens')\n",
        "                })\n",
        "        except:\n",
        "            pass\n",
        "    \n",
        "    return embedding_models\n",
        "\n",
        "# Usage\n",
        "embedding_models = list_embedding_models(\"openai\")\n",
        "for model_info in embedding_models:\n",
        "    print(f\"{model_info['model']}: {model_info['dimensions']} dimensions, ${model_info['cost_per_token']} per token\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
