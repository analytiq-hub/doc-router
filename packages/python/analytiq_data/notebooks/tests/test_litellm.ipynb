{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.append(\"../../..\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import analytiq_data as ad\n",
        "import asyncio\n",
        "import litellm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set the MONGODB_URI environment variable\n",
        "os.environ[\"MONGODB_URI\"] = \"mongodb://localhost:27017\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "litellm.model_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "litellm.model_cost[\"j2-light\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "litellm.get_model_info(model=\"groq/deepseek-r1-distill-llama-70b\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "litellm.models_by_provider"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from litellm import get_supported_openai_params\n",
        "\n",
        "response = get_supported_openai_params(model=\"bedrock/anthropic.claude-3\", custom_llm_provider=\"bedrock\")\n",
        "\n",
        "print(response) # [\"max_tokens\", \"tools\", \"tool_choice\", \"stream\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OpenAI Embedding Models:\n",
            "\n",
            "  Model: text-embedding-ada-002\n",
            "    Dimensions: 1536\n",
            "    Output cost per token: 0.0\n",
            "    Output cost per token (batches): None\n",
            "\n",
            "  Model: text-embedding-3-small\n",
            "    Dimensions: 1536\n",
            "    Output cost per token: 0.0\n",
            "    Output cost per token (batches): 0.0\n",
            "\n",
            "  Model: text-embedding-3-large\n",
            "    Dimensions: 3072\n",
            "    Output cost per token: 0.0\n",
            "    Output cost per token (batches): 0.0\n",
            "\n",
            "  Model: text-embedding-ada-002-v2\n",
            "    Dimensions: None\n",
            "    Output cost per token: 0.0\n",
            "    Output cost per token (batches): 0.0\n"
          ]
        }
      ],
      "source": [
        "def get_embedding_models(provider=\"openai\"):\n",
        "    \"\"\"\n",
        "    Get all embedding models for a given provider.\n",
        "    \n",
        "    Args:\n",
        "        provider: The litellm provider name (e.g., \"openai\", \"cohere\", \"azure\")\n",
        "    \n",
        "    Returns:\n",
        "        List of dictionaries, each containing:\n",
        "        - name: Model name\n",
        "        - dimensions: Embedding vector dimensions (output_vector_size)\n",
        "        - output_cost_per_token: Cost per token for output (typically 0 for embeddings)\n",
        "        - output_cost_per_token_batches: Cost per token for batched output (if available)\n",
        "    \"\"\"\n",
        "    models = litellm.models_by_provider.get(provider, [])\n",
        "    embedding_models = []\n",
        "    \n",
        "    for model in models:\n",
        "        try:\n",
        "            model_info = litellm.get_model_info(model)\n",
        "            # Check if this is an embedding model\n",
        "            if model_info.get('mode') == 'embedding':\n",
        "                embedding_models.append({\n",
        "                    'name': model,\n",
        "                    'dimensions': model_info.get('output_vector_size'),\n",
        "                    'output_cost_per_token': model_info.get('output_cost_per_token'),\n",
        "                    'output_cost_per_token_batches': model_info.get('output_cost_per_token_batches')\n",
        "                })\n",
        "        except Exception as e:\n",
        "            # Skip models that can't be queried\n",
        "            pass\n",
        "    \n",
        "    return embedding_models\n",
        "\n",
        "# Example usage\n",
        "embedding_models = get_embedding_models(\"openai\")\n",
        "print(\"OpenAI Embedding Models:\")\n",
        "for model in embedding_models:\n",
        "    print(f\"\\n  Model: {model['name']}\")\n",
        "    print(f\"    Dimensions: {model['dimensions']}\")\n",
        "    print(f\"    Output cost per token: {model['output_cost_per_token']}\")\n",
        "    print(f\"    Output cost per token (batches): {model['output_cost_per_token_batches']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OpenAI Embedding Models:\n",
            "  - text-embedding-ada-002\n",
            "  - text-embedding-3-small\n",
            "  - text-embedding-3-large\n",
            "  - text-embedding-ada-002-v2\n"
          ]
        }
      ],
      "source": [
        "# Get all OpenAI models\n",
        "openai_models = litellm.models_by_provider.get(\"openai\", [])\n",
        "\n",
        "# Check each model's info to see if it supports embeddings\n",
        "embedding_models = []\n",
        "for model in openai_models:\n",
        "    try:\n",
        "        model_info = litellm.get_model_info(model)\n",
        "        # Check if model supports embeddings (you may need to test or check litellm source)\n",
        "        # For OpenAI, embedding models typically have \"text-embedding\" in the name\n",
        "        if \"embedding\" in model.lower() or model_info.get(\"supports_embeddings\", False):\n",
        "            embedding_models.append(model)\n",
        "    except Exception as e:\n",
        "        # Skip models that can't be queried\n",
        "        pass\n",
        "\n",
        "print(\"OpenAI Embedding Models:\")\n",
        "for model in embedding_models:\n",
        "    print(f\"  - {model}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'key': 'gpt-4o-mini',\n",
              " 'max_tokens': 16384,\n",
              " 'max_input_tokens': 128000,\n",
              " 'max_output_tokens': 16384,\n",
              " 'input_cost_per_token': 1.5e-07,\n",
              " 'input_cost_per_token_flex': None,\n",
              " 'input_cost_per_token_priority': 2.5e-07,\n",
              " 'cache_creation_input_token_cost': None,\n",
              " 'cache_creation_input_token_cost_above_200k_tokens': None,\n",
              " 'cache_read_input_token_cost': 7.5e-08,\n",
              " 'cache_read_input_token_cost_above_200k_tokens': None,\n",
              " 'cache_read_input_token_cost_flex': None,\n",
              " 'cache_read_input_token_cost_priority': 1.25e-07,\n",
              " 'cache_creation_input_token_cost_above_1hr': None,\n",
              " 'input_cost_per_character': None,\n",
              " 'input_cost_per_token_above_128k_tokens': None,\n",
              " 'input_cost_per_token_above_200k_tokens': None,\n",
              " 'input_cost_per_query': None,\n",
              " 'input_cost_per_second': None,\n",
              " 'input_cost_per_audio_token': None,\n",
              " 'input_cost_per_image_token': None,\n",
              " 'input_cost_per_image': None,\n",
              " 'input_cost_per_audio_per_second': None,\n",
              " 'input_cost_per_video_per_second': None,\n",
              " 'input_cost_per_token_batches': 7.5e-08,\n",
              " 'output_cost_per_token_batches': 3e-07,\n",
              " 'output_cost_per_token': 6e-07,\n",
              " 'output_cost_per_token_flex': None,\n",
              " 'output_cost_per_token_priority': 1e-06,\n",
              " 'output_cost_per_audio_token': None,\n",
              " 'output_cost_per_character': None,\n",
              " 'output_cost_per_reasoning_token': None,\n",
              " 'output_cost_per_token_above_128k_tokens': None,\n",
              " 'output_cost_per_character_above_128k_tokens': None,\n",
              " 'output_cost_per_token_above_200k_tokens': None,\n",
              " 'output_cost_per_second': None,\n",
              " 'output_cost_per_video_per_second': None,\n",
              " 'output_cost_per_image': None,\n",
              " 'output_cost_per_image_token': None,\n",
              " 'output_vector_size': None,\n",
              " 'citation_cost_per_token': None,\n",
              " 'tiered_pricing': None,\n",
              " 'litellm_provider': 'openai',\n",
              " 'mode': 'chat',\n",
              " 'supports_system_messages': True,\n",
              " 'supports_response_schema': True,\n",
              " 'supports_vision': True,\n",
              " 'supports_function_calling': True,\n",
              " 'supports_tool_choice': True,\n",
              " 'supports_assistant_prefill': None,\n",
              " 'supports_prompt_caching': True,\n",
              " 'supports_audio_input': None,\n",
              " 'supports_audio_output': None,\n",
              " 'supports_pdf_input': True,\n",
              " 'supports_embedding_image_input': None,\n",
              " 'supports_native_streaming': None,\n",
              " 'supports_web_search': None,\n",
              " 'supports_url_context': None,\n",
              " 'supports_reasoning': None,\n",
              " 'supports_computer_use': None,\n",
              " 'search_context_cost_per_query': None,\n",
              " 'tpm': None,\n",
              " 'rpm': None,\n",
              " 'ocr_cost_per_page': None,\n",
              " 'annotation_cost_per_page': None,\n",
              " 'supported_openai_params': ['frequency_penalty',\n",
              "  'logit_bias',\n",
              "  'logprobs',\n",
              "  'top_logprobs',\n",
              "  'max_tokens',\n",
              "  'max_completion_tokens',\n",
              "  'modalities',\n",
              "  'prediction',\n",
              "  'n',\n",
              "  'presence_penalty',\n",
              "  'seed',\n",
              "  'stop',\n",
              "  'stream',\n",
              "  'stream_options',\n",
              "  'temperature',\n",
              "  'top_p',\n",
              "  'tools',\n",
              "  'tool_choice',\n",
              "  'function_call',\n",
              "  'functions',\n",
              "  'max_retries',\n",
              "  'extra_headers',\n",
              "  'parallel_tool_calls',\n",
              "  'audio',\n",
              "  'web_search_options',\n",
              "  'service_tier',\n",
              "  'safety_identifier',\n",
              "  'response_format',\n",
              "  'user']}"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Get the model card for text-embedding-3-small\n",
        "#model_card = litellm.get_model_info(\"gpt-4o-mini\")\n",
        "model_card = litellm.get_model_info(\"text-embedding-3-large\")\n",
        "model_card\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "text-embedding-ada-002: 1536 dimensions, $1e-07 per token\n",
            "text-embedding-3-small: 1536 dimensions, $2e-08 per token\n",
            "text-embedding-3-large: 3072 dimensions, $1.3e-07 per token\n",
            "text-embedding-ada-002-v2: None dimensions, $1e-07 per token\n"
          ]
        }
      ],
      "source": [
        "def list_embedding_models(provider=\"openai\"):\n",
        "    \"\"\"List all embedding models for a given provider\"\"\"\n",
        "    models = litellm.models_by_provider.get(provider, [])\n",
        "    embedding_models = []\n",
        "    \n",
        "    for model in models:\n",
        "        try:\n",
        "            model_info = litellm.get_model_info(model)\n",
        "            if model_info.get('mode') == 'embedding':\n",
        "                embedding_models.append({\n",
        "                    'model': model,\n",
        "                    'dimensions': model_info.get('output_vector_size'),\n",
        "                    'cost_per_token': model_info.get('input_cost_per_token'),\n",
        "                    'max_tokens': model_info.get('max_input_tokens')\n",
        "                })\n",
        "        except:\n",
        "            pass\n",
        "    \n",
        "    return embedding_models\n",
        "\n",
        "# Usage\n",
        "embedding_models = list_embedding_models(\"openai\")\n",
        "for model_info in embedding_models:\n",
        "    print(f\"{model_info['model']}: {model_info['dimensions']} dimensions, ${model_info['cost_per_token']} per token\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
